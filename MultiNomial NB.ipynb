{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla stock price prediction using Elon Musk Tweets with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the same algorithms as the paper (https://link.springer.com/chapter/10.1007/978-981-19-5090-2_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and split weekly Tweets into their own columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaud\\AppData\\Local\\Temp\\ipykernel_14984\\985602157.py:8: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  tweets_data = tweets_data[0].str.split(\", '\",len(tweets),expand=True)\n",
      "C:\\Users\\chaud\\AppData\\Local\\Temp\\ipykernel_14984\\985602157.py:11: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  tweets_data.set_axis(['Tweet ' + str(index + 1) for index in range(101)],axis=1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Dif</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet 1</th>\n",
       "      <th>Tweet 2</th>\n",
       "      <th>Tweet 3</th>\n",
       "      <th>Tweet 4</th>\n",
       "      <th>Tweet 5</th>\n",
       "      <th>Tweet 6</th>\n",
       "      <th>Tweet 7</th>\n",
       "      <th>...</th>\n",
       "      <th>Tweet 92</th>\n",
       "      <th>Tweet 93</th>\n",
       "      <th>Tweet 94</th>\n",
       "      <th>Tweet 95</th>\n",
       "      <th>Tweet 96</th>\n",
       "      <th>Tweet 97</th>\n",
       "      <th>Tweet 98</th>\n",
       "      <th>Tweet 99</th>\n",
       "      <th>Tweet 100</th>\n",
       "      <th>Tweet 101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.384667</td>\n",
       "      <td>-0.745333</td>\n",
       "      <td>[16, 254, 260, 1434, 79]</td>\n",
       "      <td>['@Lockyep Not allowed, according to HK regula...</td>\n",
       "      <td>We need to do one more minor rev on 8.0 and th...</td>\n",
       "      <td>Writing post now with details. Will publish on...</td>\n",
       "      <td>Major improvements to Autopilot coming with V8...</td>\n",
       "      <td>@newscientist uh oh']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.185333</td>\n",
       "      <td>-0.199334</td>\n",
       "      <td>[368, 4887]</td>\n",
       "      <td>['Finishing Autopilot blog postponed to  end o...</td>\n",
       "      <td>Loss of Falcon vehicle today during propellant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.220000</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>[851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...</td>\n",
       "      <td>['Will do some press Q&amp;amp;A on Autopilot post...</td>\n",
       "      <td>Thoughtful Op-ed in Space News much appreciate...</td>\n",
       "      <td>Will get back to Autopilot update blog tomorrow.'</td>\n",
       "      <td>@ashwin7002 @NASA @faa @AFPAA We have not rule...</td>\n",
       "      <td>Particularly trying to understand the quieter ...</td>\n",
       "      <td>Support &amp;amp; advice from @NASA, @FAA, @AFPAA ...</td>\n",
       "      <td>Important to note that this happened during a ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.094000</td>\n",
       "      <td>-0.126000</td>\n",
       "      <td>[7105, 4601]</td>\n",
       "      <td>['Scientists: Earth Endangered by New Strain o...</td>\n",
       "      <td>Climate change explained in comic book form by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.361333</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>[1083, 1242]</td>\n",
       "      <td>['Tesla Model S loses 28% after 50k miles, a M...</td>\n",
       "      <td>Comprehensive study by Autolist shows that a T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close       Dif                                           Retweets  \\\n",
       "0  13.384667 -0.745333                           [16, 254, 260, 1434, 79]   \n",
       "1  13.185333 -0.199334                                        [368, 4887]   \n",
       "2  13.220000  0.034667  [851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...   \n",
       "3  13.094000 -0.126000                                       [7105, 4601]   \n",
       "4  13.361333  0.267333                                       [1083, 1242]   \n",
       "\n",
       "                                             Tweet 1  \\\n",
       "0  ['@Lockyep Not allowed, according to HK regula...   \n",
       "1  ['Finishing Autopilot blog postponed to  end o...   \n",
       "2  ['Will do some press Q&amp;A on Autopilot post...   \n",
       "3  ['Scientists: Earth Endangered by New Strain o...   \n",
       "4  ['Tesla Model S loses 28% after 50k miles, a M...   \n",
       "\n",
       "                                             Tweet 2  \\\n",
       "0  We need to do one more minor rev on 8.0 and th...   \n",
       "1  Loss of Falcon vehicle today during propellant...   \n",
       "2  Thoughtful Op-ed in Space News much appreciate...   \n",
       "3  Climate change explained in comic book form by...   \n",
       "4  Comprehensive study by Autolist shows that a T...   \n",
       "\n",
       "                                             Tweet 3  \\\n",
       "0  Writing post now with details. Will publish on...   \n",
       "1                                                NaN   \n",
       "2  Will get back to Autopilot update blog tomorrow.'   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 4  \\\n",
       "0  Major improvements to Autopilot coming with V8...   \n",
       "1                                                NaN   \n",
       "2  @ashwin7002 @NASA @faa @AFPAA We have not rule...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 5  \\\n",
       "0                              @newscientist uh oh']   \n",
       "1                                                NaN   \n",
       "2  Particularly trying to understand the quieter ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 6  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Support &amp; advice from @NASA, @FAA, @AFPAA ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 7  ... Tweet 92 Tweet 93  \\\n",
       "0                                                NaN  ...      NaN      NaN   \n",
       "1                                                NaN  ...      NaN      NaN   \n",
       "2  Important to note that this happened during a ...  ...      NaN      NaN   \n",
       "3                                                NaN  ...      NaN      NaN   \n",
       "4                                                NaN  ...      NaN      NaN   \n",
       "\n",
       "  Tweet 94 Tweet 95 Tweet 96 Tweet 97 Tweet 98 Tweet 99 Tweet 100 Tweet 101  \n",
       "0      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN  \n",
       "1      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN  \n",
       "2      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN  \n",
       "3      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN  \n",
       "4      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "dataset = pd.read_csv(\"results.csv\")\n",
    "tweets = dataset['Tweets'].tolist()\n",
    "tweets_data = pd.DataFrame(tweets)\n",
    "tweets_data = tweets_data[0].str.split(\", '\",len(tweets),expand=True)\n",
    "\n",
    "\n",
    "tweets_data.set_axis(['Tweet ' + str(index + 1) for index in range(101)],axis=1,inplace=True)  \n",
    "dataset = pd.concat([dataset,tweets_data],axis=1,join='inner')\n",
    "dataset.drop(dataset.columns[0],axis=1,inplace=True)\n",
    "dataset.drop(['Likes','Tweets'],axis=1,inplace=True)\n",
    "dataset.fillna(value=np.nan,inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give each week a Sentiment based on if it resulted in the a increase or decrease in stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaud\\AppData\\Local\\Temp\\ipykernel_14984\\190792637.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset['Sentiment'] = sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Dif</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet 1</th>\n",
       "      <th>Tweet 2</th>\n",
       "      <th>Tweet 3</th>\n",
       "      <th>Tweet 4</th>\n",
       "      <th>Tweet 5</th>\n",
       "      <th>Tweet 6</th>\n",
       "      <th>Tweet 7</th>\n",
       "      <th>...</th>\n",
       "      <th>Tweet 93</th>\n",
       "      <th>Tweet 94</th>\n",
       "      <th>Tweet 95</th>\n",
       "      <th>Tweet 96</th>\n",
       "      <th>Tweet 97</th>\n",
       "      <th>Tweet 98</th>\n",
       "      <th>Tweet 99</th>\n",
       "      <th>Tweet 100</th>\n",
       "      <th>Tweet 101</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.384667</td>\n",
       "      <td>-0.745333</td>\n",
       "      <td>[16, 254, 260, 1434, 79]</td>\n",
       "      <td>['@Lockyep Not allowed, according to HK regula...</td>\n",
       "      <td>We need to do one more minor rev on 8.0 and th...</td>\n",
       "      <td>Writing post now with details. Will publish on...</td>\n",
       "      <td>Major improvements to Autopilot coming with V8...</td>\n",
       "      <td>@newscientist uh oh']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.185333</td>\n",
       "      <td>-0.199334</td>\n",
       "      <td>[368, 4887]</td>\n",
       "      <td>['Finishing Autopilot blog postponed to  end o...</td>\n",
       "      <td>Loss of Falcon vehicle today during propellant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.220000</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>[851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...</td>\n",
       "      <td>['Will do some press Q&amp;amp;A on Autopilot post...</td>\n",
       "      <td>Thoughtful Op-ed in Space News much appreciate...</td>\n",
       "      <td>Will get back to Autopilot update blog tomorrow.'</td>\n",
       "      <td>@ashwin7002 @NASA @faa @AFPAA We have not rule...</td>\n",
       "      <td>Particularly trying to understand the quieter ...</td>\n",
       "      <td>Support &amp;amp; advice from @NASA, @FAA, @AFPAA ...</td>\n",
       "      <td>Important to note that this happened during a ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.094000</td>\n",
       "      <td>-0.126000</td>\n",
       "      <td>[7105, 4601]</td>\n",
       "      <td>['Scientists: Earth Endangered by New Strain o...</td>\n",
       "      <td>Climate change explained in comic book form by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.361333</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>[1083, 1242]</td>\n",
       "      <td>['Tesla Model S loses 28% after 50k miles, a M...</td>\n",
       "      <td>Comprehensive study by Autolist shows that a T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close       Dif                                           Retweets  \\\n",
       "0  13.384667 -0.745333                           [16, 254, 260, 1434, 79]   \n",
       "1  13.185333 -0.199334                                        [368, 4887]   \n",
       "2  13.220000  0.034667  [851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...   \n",
       "3  13.094000 -0.126000                                       [7105, 4601]   \n",
       "4  13.361333  0.267333                                       [1083, 1242]   \n",
       "\n",
       "                                             Tweet 1  \\\n",
       "0  ['@Lockyep Not allowed, according to HK regula...   \n",
       "1  ['Finishing Autopilot blog postponed to  end o...   \n",
       "2  ['Will do some press Q&amp;A on Autopilot post...   \n",
       "3  ['Scientists: Earth Endangered by New Strain o...   \n",
       "4  ['Tesla Model S loses 28% after 50k miles, a M...   \n",
       "\n",
       "                                             Tweet 2  \\\n",
       "0  We need to do one more minor rev on 8.0 and th...   \n",
       "1  Loss of Falcon vehicle today during propellant...   \n",
       "2  Thoughtful Op-ed in Space News much appreciate...   \n",
       "3  Climate change explained in comic book form by...   \n",
       "4  Comprehensive study by Autolist shows that a T...   \n",
       "\n",
       "                                             Tweet 3  \\\n",
       "0  Writing post now with details. Will publish on...   \n",
       "1                                                NaN   \n",
       "2  Will get back to Autopilot update blog tomorrow.'   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 4  \\\n",
       "0  Major improvements to Autopilot coming with V8...   \n",
       "1                                                NaN   \n",
       "2  @ashwin7002 @NASA @faa @AFPAA We have not rule...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 5  \\\n",
       "0                              @newscientist uh oh']   \n",
       "1                                                NaN   \n",
       "2  Particularly trying to understand the quieter ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 6  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Support &amp; advice from @NASA, @FAA, @AFPAA ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 7  ... Tweet 93 Tweet 94  \\\n",
       "0                                                NaN  ...      NaN      NaN   \n",
       "1                                                NaN  ...      NaN      NaN   \n",
       "2  Important to note that this happened during a ...  ...      NaN      NaN   \n",
       "3                                                NaN  ...      NaN      NaN   \n",
       "4                                                NaN  ...      NaN      NaN   \n",
       "\n",
       "  Tweet 95 Tweet 96 Tweet 97 Tweet 98 Tweet 99 Tweet 100 Tweet 101 Sentiment  \n",
       "0      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "1      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "2      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "3      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "4      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def giveSentimentRanges(cell):\n",
    "    #if (-147.75 <= cell <= -79.31):\n",
    "      #  return 0\n",
    "    #elif(-79.30 <= cell <= -10.86):\n",
    "       # return 1\n",
    "    #elif(-10.85 <= cell <= 57.59):\n",
    "        #return 2\n",
    "    #elif(57.60 <= cell <= 126.04):\n",
    "       # return 3\n",
    "    #elif(126.05 <= cell <= 194.49):\n",
    "      #  return 4 \n",
    "\n",
    "def giveSentiment(cell):\n",
    "    if (cell <= 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "sentiment = list()\n",
    "\n",
    "for diff in dataset['Dif']:\n",
    "    sentiment.append(giveSentiment(diff))\n",
    "\n",
    "dataset['Sentiment'] = sentiment\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup the data by removing unnecessary characters from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Dif</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet 1</th>\n",
       "      <th>Tweet 2</th>\n",
       "      <th>Tweet 3</th>\n",
       "      <th>Tweet 4</th>\n",
       "      <th>Tweet 5</th>\n",
       "      <th>Tweet 6</th>\n",
       "      <th>Tweet 7</th>\n",
       "      <th>...</th>\n",
       "      <th>Tweet 93</th>\n",
       "      <th>Tweet 94</th>\n",
       "      <th>Tweet 95</th>\n",
       "      <th>Tweet 96</th>\n",
       "      <th>Tweet 97</th>\n",
       "      <th>Tweet 98</th>\n",
       "      <th>Tweet 99</th>\n",
       "      <th>Tweet 100</th>\n",
       "      <th>Tweet 101</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.384667</td>\n",
       "      <td>-0.745333</td>\n",
       "      <td>[16, 254, 260, 1434, 79]</td>\n",
       "      <td>not allowed, according to hk regulations. happ...</td>\n",
       "      <td>we need to do one more minor rev on . and then...</td>\n",
       "      <td>writing post now with details. will publish on...</td>\n",
       "      <td>major improvements to autopilot coming with v....</td>\n",
       "      <td>uh oh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.185333</td>\n",
       "      <td>-0.199334</td>\n",
       "      <td>[368, 4887]</td>\n",
       "      <td>finishing autopilot blog postponed to  end of ...</td>\n",
       "      <td>loss of falcon vehicle today during propellant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.220000</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>[851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...</td>\n",
       "      <td>will do some press q on autopilot post at am p...</td>\n",
       "      <td>thoughtful op-ed in space news much appreciate...</td>\n",
       "      <td>will get back to autopilot update blog tomorrow.</td>\n",
       "      <td>we have not ruled that out.,  nope, it wasnt me</td>\n",
       "      <td>particularly trying to understand the quieter ...</td>\n",
       "      <td>support  advice from     others much appreciat...</td>\n",
       "      <td>important to note that this happened during a ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.094000</td>\n",
       "      <td>-0.126000</td>\n",
       "      <td>[7105, 4601]</td>\n",
       "      <td>scientists: earth endangered by new strain of ...</td>\n",
       "      <td>climate change explained in comic book form by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.361333</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>[1083, 1242]</td>\n",
       "      <td>tesla model s loses % after k miles, a mercede...</td>\n",
       "      <td>comprehensive study by autolist shows that a t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close       Dif                                           Retweets  \\\n",
       "0  13.384667 -0.745333                           [16, 254, 260, 1434, 79]   \n",
       "1  13.185333 -0.199334                                        [368, 4887]   \n",
       "2  13.220000  0.034667  [851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...   \n",
       "3  13.094000 -0.126000                                       [7105, 4601]   \n",
       "4  13.361333  0.267333                                       [1083, 1242]   \n",
       "\n",
       "                                             Tweet 1  \\\n",
       "0  not allowed, according to hk regulations. happ...   \n",
       "1  finishing autopilot blog postponed to  end of ...   \n",
       "2  will do some press q on autopilot post at am p...   \n",
       "3  scientists: earth endangered by new strain of ...   \n",
       "4  tesla model s loses % after k miles, a mercede...   \n",
       "\n",
       "                                             Tweet 2  \\\n",
       "0  we need to do one more minor rev on . and then...   \n",
       "1  loss of falcon vehicle today during propellant...   \n",
       "2  thoughtful op-ed in space news much appreciate...   \n",
       "3  climate change explained in comic book form by...   \n",
       "4  comprehensive study by autolist shows that a t...   \n",
       "\n",
       "                                             Tweet 3  \\\n",
       "0  writing post now with details. will publish on...   \n",
       "1                                                NaN   \n",
       "2   will get back to autopilot update blog tomorrow.   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 4  \\\n",
       "0  major improvements to autopilot coming with v....   \n",
       "1                                                NaN   \n",
       "2    we have not ruled that out.,  nope, it wasnt me   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 5  \\\n",
       "0                                              uh oh   \n",
       "1                                                NaN   \n",
       "2  particularly trying to understand the quieter ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 6  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  support  advice from     others much appreciat...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 7  ... Tweet 93 Tweet 94  \\\n",
       "0                                                NaN  ...      NaN      NaN   \n",
       "1                                                NaN  ...      NaN      NaN   \n",
       "2  important to note that this happened during a ...  ...      NaN      NaN   \n",
       "3                                                NaN  ...      NaN      NaN   \n",
       "4                                                NaN  ...      NaN      NaN   \n",
       "\n",
       "  Tweet 95 Tweet 96 Tweet 97 Tweet 98 Tweet 99 Tweet 100 Tweet 101 Sentiment  \n",
       "0      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "1      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "2      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "3      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "4      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in dataset.columns[3:-1]:\n",
    "    dataset[column] = dataset[column].str.lower()\n",
    "    dataset[column] = dataset[column].replace('((\\\\n\\()?\\@\\S+)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('((\\\\n)?https\\S+)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\[)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\])','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\&\\S+)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\#)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\_)','',regex=True)\n",
    "    dataset[column] = dataset[column].str.replace('([^\\w\\s#@/:%.,_-])','',regex=True,flags=re.UNICODE)\n",
    "    dataset[column] = dataset[column].replace('([^\\x00-\\x7FâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]+)','',regex=True)\n",
    "    dataset[column] = dataset[column].replace('(\\d+)','',regex=True)\n",
    "    dataset[column] = dataset[column].str.lstrip()\n",
    "dataset = dataset.mask(dataset == '')\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization of the tweets\n",
    "\n",
    "Note: you must uncomment the \"nltk.download\" lines the first time you run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Dif</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet 1</th>\n",
       "      <th>Tweet 2</th>\n",
       "      <th>Tweet 3</th>\n",
       "      <th>Tweet 4</th>\n",
       "      <th>Tweet 5</th>\n",
       "      <th>Tweet 6</th>\n",
       "      <th>Tweet 7</th>\n",
       "      <th>...</th>\n",
       "      <th>Tweet 93</th>\n",
       "      <th>Tweet 94</th>\n",
       "      <th>Tweet 95</th>\n",
       "      <th>Tweet 96</th>\n",
       "      <th>Tweet 97</th>\n",
       "      <th>Tweet 98</th>\n",
       "      <th>Tweet 99</th>\n",
       "      <th>Tweet 100</th>\n",
       "      <th>Tweet 101</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.384667</td>\n",
       "      <td>-0.745333</td>\n",
       "      <td>[16, 254, 260, 1434, 79]</td>\n",
       "      <td>not allowed, according to hk regulations. happ...</td>\n",
       "      <td>we need to do one more minor rev on . and then...</td>\n",
       "      <td>writing post now with details. will publish on...</td>\n",
       "      <td>major improvements to autopilot coming with v....</td>\n",
       "      <td>uh oh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.185333</td>\n",
       "      <td>-0.199334</td>\n",
       "      <td>[368, 4887]</td>\n",
       "      <td>finishing autopilot blog postponed to  end of ...</td>\n",
       "      <td>loss of falcon vehicle today during propellant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.220000</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>[851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...</td>\n",
       "      <td>will do some press q on autopilot post at am p...</td>\n",
       "      <td>thoughtful op-ed in space news much appreciate...</td>\n",
       "      <td>will get back to autopilot update blog tomorrow.</td>\n",
       "      <td>we have not ruled that out.,  nope, it wasnt me</td>\n",
       "      <td>particularly trying to understand the quieter ...</td>\n",
       "      <td>support  advice from     others much appreciat...</td>\n",
       "      <td>important to note that this happened during a ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.094000</td>\n",
       "      <td>-0.126000</td>\n",
       "      <td>[7105, 4601]</td>\n",
       "      <td>scientists: earth endangered by new strain of ...</td>\n",
       "      <td>climate change explained in comic book form by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.361333</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>[1083, 1242]</td>\n",
       "      <td>tesla model s loses % after k miles, a mercede...</td>\n",
       "      <td>comprehensive study by autolist shows that a t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close       Dif                                           Retweets  \\\n",
       "0  13.384667 -0.745333                           [16, 254, 260, 1434, 79]   \n",
       "1  13.185333 -0.199334                                        [368, 4887]   \n",
       "2  13.220000  0.034667  [851, 1406, 23, 247, 93, 89, 683, 840, 1223, 2...   \n",
       "3  13.094000 -0.126000                                       [7105, 4601]   \n",
       "4  13.361333  0.267333                                       [1083, 1242]   \n",
       "\n",
       "                                             Tweet 1  \\\n",
       "0  not allowed, according to hk regulations. happ...   \n",
       "1  finishing autopilot blog postponed to  end of ...   \n",
       "2  will do some press q on autopilot post at am p...   \n",
       "3  scientists: earth endangered by new strain of ...   \n",
       "4  tesla model s loses % after k miles, a mercede...   \n",
       "\n",
       "                                             Tweet 2  \\\n",
       "0  we need to do one more minor rev on . and then...   \n",
       "1  loss of falcon vehicle today during propellant...   \n",
       "2  thoughtful op-ed in space news much appreciate...   \n",
       "3  climate change explained in comic book form by...   \n",
       "4  comprehensive study by autolist shows that a t...   \n",
       "\n",
       "                                             Tweet 3  \\\n",
       "0  writing post now with details. will publish on...   \n",
       "1                                                NaN   \n",
       "2   will get back to autopilot update blog tomorrow.   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 4  \\\n",
       "0  major improvements to autopilot coming with v....   \n",
       "1                                                NaN   \n",
       "2    we have not ruled that out.,  nope, it wasnt me   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 5  \\\n",
       "0                                              uh oh   \n",
       "1                                                NaN   \n",
       "2  particularly trying to understand the quieter ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 6  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  support  advice from     others much appreciat...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Tweet 7  ... Tweet 93 Tweet 94  \\\n",
       "0                                                NaN  ...      NaN      NaN   \n",
       "1                                                NaN  ...      NaN      NaN   \n",
       "2  important to note that this happened during a ...  ...      NaN      NaN   \n",
       "3                                                NaN  ...      NaN      NaN   \n",
       "4                                                NaN  ...      NaN      NaN   \n",
       "\n",
       "  Tweet 95 Tweet 96 Tweet 97 Tweet 98 Tweet 99 Tweet 100 Tweet 101 Sentiment  \n",
       "0      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "1      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "2      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "3      NaN      NaN      NaN      NaN      NaN       NaN       NaN         0  \n",
       "4      NaN      NaN      NaN      NaN      NaN       NaN       NaN         1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "#Note- YOU WILL NEED TO UNCOMMENT THESE 3 LINES FOR THE CODE TO WORK\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    stem_sentence=list()\n",
    "    for word in words:\n",
    "        stem_sentence.append(stem.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "def LemmatizerSentence(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    lemma_sentence=list()\n",
    "    for word in words:\n",
    "        lemma_sentence.append(lemma.lemmatize(word))\n",
    "        lemma_sentence.append(\" \")\n",
    "    return \"\".join(lemma_sentence)\n",
    "\n",
    "for column in dataset.columns[3:-1]:\n",
    "    result = list()\n",
    "    for row in dataset[column]: \n",
    "        if(type(row) != float):\n",
    "            result.append(LemmatizerSentence(stemSentence(row)))\n",
    "\n",
    "    dataset[column].loc[len(dataset[column])] = result\n",
    "    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-Vectorization of the tweets to convert them to numerical arrays that can be used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114, 12017)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "each_row = dict()\n",
    "all_tweets = ''\n",
    "for row in dataset.index:\n",
    "    row_tweets = ''\n",
    "    for column in dataset.columns[3:-1]:\n",
    "        if(type(dataset[column][row]) is str):\n",
    "            row_tweets += dataset[column][row] + ' '\n",
    "            all_tweets += dataset[column][row] + ' '\n",
    "    each_row[row] = [row_tweets]\n",
    "\n",
    "count_vector = list()\n",
    "vectorizer.fit([all_tweets])\n",
    "for keys in each_row:\n",
    "    vector = vectorizer.transform(each_row[keys])\n",
    "    count_vector.append(vector.toarray())\n",
    "\n",
    "count_vector = np.reshape(count_vector,(np.shape(count_vector)[0],np.shape(count_vector)[2]))\n",
    "print(np.shape(count_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset to be used as Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaud\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aargh</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>aber</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablate</th>\n",
       "      <th>ablative</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zorbix</th>\n",
       "      <th>zork</th>\n",
       "      <th>zu</th>\n",
       "      <th>zx</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaargh  aargh  abandoned  abandoning  abdomen  aber  abide  ability  \\\n",
       "0       0      0          0           0        0     0      0        0   \n",
       "1       0      0          0           0        0     0      0        0   \n",
       "2       0      0          0           0        0     0      0        0   \n",
       "3       0      0          0           0        0     0      0        0   \n",
       "4       0      0          0           0        0     0      0        0   \n",
       "\n",
       "   ablate  ablative  ...  zone  zones  zooming  zooms  zootopia  zorbix  zork  \\\n",
       "0       0         0  ...     0      0        0      0         0       0     0   \n",
       "1       0         0  ...     0      0        0      0         0       0     0   \n",
       "2       0         0  ...     0      0        0      0         0       0     0   \n",
       "3       0         0  ...     0      0        0      0         0       0     0   \n",
       "4       0         0  ...     0      0        0      0         0       0     0   \n",
       "\n",
       "   zu  zx  Sentiment  \n",
       "0   0   0          0  \n",
       "1   0   0          0  \n",
       "2   0   0          1  \n",
       "3   0   0          0  \n",
       "4   0   0          1  \n",
       "\n",
       "[5 rows x 12018 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Word_occurances = pd.DataFrame()\n",
    "\n",
    "Word_occurances= pd.DataFrame(count_vector, columns=vectorizer.get_feature_names())\n",
    "Word_occurances['Sentiment'] = dataset['Sentiment']\n",
    "\n",
    "Word_occurances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into train and test + training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "words = Word_occurances.iloc[:,:-1]\n",
    "X_train,X_test,y_train,y_test = train_test_split(words,Word_occurances['Sentiment'],test_size=0.2,random_state=4)\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,y_train)\n",
    "\n",
    "prediction = MNB.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5650224215246636\n",
      "Precision:  0.5899280575539568\n",
      "F1 Score:  0.6283524904214559\n",
      "Confusion Matrix: \n",
      " [[44 57]\n",
      " [40 82]]\n",
      "\n",
      "\n",
      " Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.44      0.48       101\n",
      "           1       0.59      0.67      0.63       122\n",
      "\n",
      "    accuracy                           0.57       223\n",
      "   macro avg       0.56      0.55      0.55       223\n",
      "weighted avg       0.56      0.57      0.56       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test,prediction))\n",
    "print(\"Precision: \",metrics.precision_score(y_test,prediction))\n",
    "print(\"F1 Score: \",metrics.f1_score(y_test,prediction))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(y_test,prediction))\n",
    "print(\"\\n\\n Classification Report: \\n\", metrics.classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification applied where the setiment scores given are now based on mean and deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aargh</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>aber</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>ablate</th>\n",
       "      <th>ablative</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zorbix</th>\n",
       "      <th>zork</th>\n",
       "      <th>zu</th>\n",
       "      <th>zx</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Ranged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12019 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaargh  aargh  abandoned  abandoning  abdomen  aber  abide  ability  \\\n",
       "0       0      0          0           0        0     0      0        0   \n",
       "1       0      0          0           0        0     0      0        0   \n",
       "2       0      0          0           0        0     0      0        0   \n",
       "3       0      0          0           0        0     0      0        0   \n",
       "4       0      0          0           0        0     0      0        0   \n",
       "\n",
       "   ablate  ablative  ...  zones  zooming  zooms  zootopia  zorbix  zork  zu  \\\n",
       "0       0         0  ...      0        0      0         0       0     0   0   \n",
       "1       0         0  ...      0        0      0         0       0     0   0   \n",
       "2       0         0  ...      0        0      0         0       0     0   0   \n",
       "3       0         0  ...      0        0      0         0       0     0   0   \n",
       "4       0         0  ...      0        0      0         0       0     0   0   \n",
       "\n",
       "   zx  Sentiment  Sentiment_Ranged  \n",
       "0   0          0                 1  \n",
       "1   0          0                 1  \n",
       "2   0          1                 1  \n",
       "3   0          0                 1  \n",
       "4   0          1                 2  \n",
       "\n",
       "[5 rows x 12019 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import stdev\n",
    "\n",
    "def giveSentimentRanges(cell):\n",
    "    if (cell < mean - deviation):\n",
    "        return 0\n",
    "    elif(mean - deviation <= cell < mean):\n",
    "        return 1\n",
    "    elif(mean <= cell < mean + (deviation)):\n",
    "        return 2\n",
    "    elif(mean + (deviation) <= cell < mean + (deviation*2)):\n",
    "        return 3\n",
    "    elif(cell > mean + (deviation*2)):\n",
    "        return 4 \n",
    "\n",
    "deviation = stdev(dataset['Dif'])\n",
    "mean = dataset['Dif'].mean()\n",
    "range_sentiment = list()\n",
    "for diff in dataset['Dif']:\n",
    "    range_sentiment.append(giveSentimentRanges(diff))\n",
    "\n",
    "Word_occurances['Sentiment_Ranged'] = range_sentiment\n",
    "Word_occurances.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for the modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4125560538116592\n",
      "Precision:  0.3414395768655858\n",
      "F1 Score:  0.36214116733166196\n",
      "Confusion Matrix: \n",
      " [[ 0 16  1  0  0]\n",
      " [ 2 65 31  0  0]\n",
      " [ 0 58 27  0  0]\n",
      " [ 0  9  4  0  0]\n",
      " [ 0  8  2  0  0]]\n",
      "\n",
      "\n",
      " Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.42      0.66      0.51        98\n",
      "           2       0.42      0.32      0.36        85\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.41       223\n",
      "   macro avg       0.17      0.20      0.17       223\n",
      "weighted avg       0.34      0.41      0.36       223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaud\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\chaud\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\chaud\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\chaud\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(words,Word_occurances['Sentiment_Ranged'],test_size=0.2,random_state=4)\n",
    "\n",
    "MNB_ranges = MultinomialNB()\n",
    "MNB_ranges.fit(X_train,y_train)\n",
    "\n",
    "ranges_prediction = MNB_ranges.predict(X_test)\n",
    "#print(y_test)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test,ranges_prediction))\n",
    "print(\"Precision: \",metrics.precision_score(y_test,ranges_prediction,average='weighted'))\n",
    "print(\"F1 Score: \",metrics.f1_score(y_test,ranges_prediction,average='weighted'))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(y_test,ranges_prediction))\n",
    "print(\"\\n\\n Classification Report: \\n\", metrics.classification_report(y_test,ranges_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ebf7fc8527880daa6e4363d8814dee48eb0f49c868bbf822a44ec45023d0532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
