{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#list of words to filter out of statements\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to clean and generate sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import ast\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# stemmer to turn words into their roots\n",
    "lan=LancasterStemmer()\n",
    "\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|ftp\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub('(@Tesla)','Tesla',tweet)\n",
    "    tweet = re.sub('(&amp)','',tweet)\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    tweet = re.sub('[^\\w ]','',tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet_tokens = word_tokenize(tweet,preserve_line=True)\n",
    "    filtered_words = [lan.stem(w) for w in tweet_tokens if not w in stop_words]\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def avg(data):\n",
    "    data = ast.literal_eval(data)\n",
    "    sum = 0\n",
    "    for item in data:\n",
    "        sum += item\n",
    "    return sum/len(data)\n",
    "\n",
    "def getSIA(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "def vader(row):\n",
    "    tweets = ast.literal_eval(row['Tweets'])\n",
    "    pos= []\n",
    "    neg= []\n",
    "    neu= []\n",
    "    for tweet in tweets:\n",
    "        SIA = getSIA(preprocess_tweet_text(tweet))\n",
    "        pos.append(SIA['pos'])\n",
    "        neg.append(SIA['neu'])\n",
    "        neu.append(SIA['neg'])\n",
    "    return np.average(pos), np.average(neg), np.average(neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv and turn data call helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"results.csv\")\n",
    "\n",
    "df['pos'] = df.apply(vader,axis=1)\n",
    "df[['pos', 'neg', 'neu']] = pd.DataFrame(df['pos'].tolist(), index=df.index)\n",
    "df['Likes'] = df['Likes'].apply(avg)\n",
    "df['label'] = (df['Dif'] > 0) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter columns for features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pos', 'neu', 'neg', 'label', 'Likes'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "keep = ['pos', 'neu','neg','label', 'Likes']\n",
    "filtered = df[keep]\n",
    "print(filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = filtered.loc[ : , filtered.columns != 'label']\n",
    "y = filtered['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(X , y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model, and generate the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6143497757847534\n",
      "F-score is: 0.6814814814814815\n",
      "Area Under Curve: 0.6017799352750809\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Price Drop       0.62      0.44      0.51       103\n",
      "  Price Rise       0.61      0.77      0.68       120\n",
      "\n",
      "    accuracy                           0.61       223\n",
      "   macro avg       0.61      0.60      0.60       223\n",
      "weighted avg       0.61      0.61      0.60       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, f1_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=190, random_state=36).fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "print(\"Accuracy is:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"F-score is:\", f1_score(y_test, y_pred))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "print(\"Area Under Curve:\", auc(fpr, tpr))\n",
    "\n",
    "classificationReport = classification_report(y_test, y_pred, target_names=['Price Drop', 'Price Rise'])\n",
    "print(\"\\nClassification Report:\\n\", classificationReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7af9e86c3681899aba80f8b0067881b136bce65c27e78bb175258adc8d4bb5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
